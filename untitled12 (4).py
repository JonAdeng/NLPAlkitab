# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19yWrpEhAkiz-RTHWM76iStIiEa_v0qPy
"""

!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py
!pip install tokenization
!pip install nltk
!pip install Sastrawi
!pip install transformers

import numpy as np
import pandas as pd
import nltk
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

from collections import Counter, defaultdict
import tokenization
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

import torch
from transformers import BertTokenizer, BertModel

# import numpy as np
# import pandas as pd
# import nltk
# nltk.download('wordnet')
# nltk.download('stopwords')
# nltk.download('punkt')
# from nltk.stem import PorterStemmer, WordNetLemmatizer
# from nltk.tokenize import sent_tokenize, word_tokenize
# from nltk.corpus import stopwords
# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
# from Sastrawi.StopWordRemover import StopWordRemover


# from collections import Counter, defaultdict
# import tokenization
# import string

# import matplotlib as mpl
# import matplotlib.pyplot as plt
# import seaborn as sns
# from wordcloud import WordCloud

# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.decomposition import TruncatedSVD
# from sklearn.model_selection import train_test_split
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# import operator

# import tensorflow as tf
# import tensorflow_hub as hub
# from tensorflow import keras
# from tensorflow.keras.optimizers import SGD, Adam
# from tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D
# from tensorflow.keras.models import Model, Sequential
# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback

# import torch
# from transformers import BertTokenizer, BertModel


# import gc

from google.colab import drive
drive.mount('/content/gdrive')

tb=pd.read_csv('/content/gdrive/MyDrive/tb_update.csv')
print("The number of rows: " + format(tb.shape[0]) + " The number of factors: " + format(tb.shape[1]))
tb.head()

tb = tb[(tb['id_kitab'] == 20) | (tb['id_kitab'] == 44)]
tb.tail()

df_plus = pd.read_csv('/content/gdrive/MyDrive/BibleBooks.csv', encoding ='latin-1')
df_plus = df_plus[(df_plus['Book'] == 'Amsal') | (df_plus['Book'] == 'Kisah Para Rasul')]
print("The number of rows: " + format(df_plus.shape[0]) + " The number of factors: " + format(df_plus.shape[1]))
df_plus.head()

tb['firman'] = tb['firman'].astype('str')
tb.loc[tb['id_kitab'] <= 39, 'Perjanjian'] = 'Lama'
tb.loc[tb['id_kitab'] > 39, 'Perjanjian'] = 'Baru'
tb

df = df_plus.drop(['Tanakh','New Jerusalem Version'], axis=1)
df = df.rename(columns={"Book":"Kitab", "King James Version":"Versi_KingJames"})
df['Versi_KingJames']=df ['Versi_KingJames'].replace(np.nan, 0)
df['Versi_KingJames'] = df['Versi_KingJames'].astype('int')
df

def lower_column_t(data):
    values = data['firman']
    values = values.lower()
    data['firman'] = values
    return data

def clean_interpunction(data):
    values = data['firman']
    values = values.replace('.','')
    values = values.replace(';','')
    values = values.replace(':','')
    values = values.replace(',','')
    values = values.replace("'","")
    values = values.replace('"','')
    data['firman'] = values
    return data

stop_words = stopwords.words('indonesian')
additional_stopwords = ['yang', 'dan']
stop_words.extend(additional_stopwords)

def clean_text(text, stop_words):
    # Tokenisasi teks
    words = nltk.word_tokenize(text)

    # Menghapus stopwords
    words = [word for word in words if word.lower() not in stop_words]

    # Menggabungkan kembali menjadi teks
    cleaned_text = ' '.join(words)
    return cleaned_text

def remove_stopwords(text, stop_words):
    words = nltk.word_tokenize(text)
    words = [word for word in words if word.lower() not in stop_words]
    return ' '.join(words)


def sastrawi_stem(text):
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()
    return stemmer.stem(text)

def sastrawi_lem(text):
    return sastrawi_stem(text)

def tokenize(a):
    b = []
    for line in a:
        b.append(word_tokenize(line))

    return b

def flatten(a):
    b = []
    for line in a:
        b = ' '.join(line)

    return b

def count_words(a):
    b=0
    for line in a:
        b = b + sum([i.strip(string.punctuation).isalpha() for i in line.split()])

    return b

def generate_ngrams(text, n_gram=1):
    token = [token for token in text.lower().split(' ') if token != '' if token not in sw]
    ngrams = zip(*[token[i:] for i in range(n_gram)])
    return [' '.join(ngram) for ngram in ngrams]

tb = tb.apply(lower_column_t, axis=1)  # Mengubah ke huruf kecil
tb = tb.apply(clean_interpunction, axis=1)  # Membersihkan tanda baca
tb['firman'] = tb['firman'].apply(lambda x: remove_stopwords(x, stop_words))
tb['t_stem'] = tb['firman'].apply(sastrawi_stem)
tb['t_stem_lem'] = tb['t_stem'].apply(sastrawi_lem)
tb['NoWords'] = tb['firman'].str.split().str.len()
tb = tb.set_index('id')
tb

tb.to_csv('dataset_olahan.csv', index=False)

tb_plus=pd.read_csv('/content/dataset_olahan.csv')
tb_plus = tb_plus.merge(df, left_on='id_kitab', right_on='Versi_KingJames')
tb_plus

color_1 = plt.cm.Blues(np.linspace(0.6, 1, 66))
color_2 = plt.cm.Purples(np.linspace(0.6, 1, 66))

panjang_bab = tb_plus.groupby('kitab').agg({'pasal': 'count', 'NoWords': 'sum'}).sort_values(by='pasal', ascending=False)
data1 = panjang_bab['pasal']
data2 = panjang_bab['NoWords']

plt.figure(figsize=(16, 8))
x = np.arange(len(panjang_bab))

ax1 = plt.subplot(1, 1, 1)
w = 0.3
plt.xticks(x + w / 2, panjang_bab.index, rotation='vertical')
pop = ax1.bar(x, data1, width=w, label='Verse', color=color_1)

ax2 = ax1.twinx()
gdp = ax2.bar(x + w, data2, width=w, label='Kata', color=color_2)

plt.title('Jumlah kata dalam suatu ayat')
ax1.set_ylabel('Ayat')
ax2.set_ylabel('Kata')
plt.legend([pop, gdp], ['Ayat', 'Kata'])
plt.show()

color_1 = plt.cm.Blues(np.linspace(0.6, 1, 66))
color_2 = plt.cm.Purples(np.linspace(0.6, 1, 66))

panjang_bab_ = tb_plus.groupby('Waktu').agg({'id_kitab': 'mean', 'NoWords': 'mean', 'Waktu': 'mean'})
data1 = panjang_bab_['id_kitab']
data2 = panjang_bab_['NoWords']

plt.figure(figsize=(16, 8))
x = np.arange(len(data1.index))

ax1 = plt.subplot(1, 1, 1)
w = 0.3

plt.title('Jumlah kata dan jumlah ayat')
plt.xticks(x + w / 2, data1.index, rotation=-90)
ax1.set_xlabel('Waktu')
ax1.set_ylabel('Jumlah ayat')
ax1.bar(x, data1.values, color=color_1, width=w, align='center')

ax2 = ax1.twinx()
ax2.set_ylabel('Jumlah kata')
ax2.bar(x + w, data2.values, color=color_2, width=w, align='center')

plt.show()

# Convert columns to string
tb_plus['firman'] = tb_plus['firman'].astype(str)
tb_plus['t_stem'] = tb_plus['t_stem'].astype(str)

sw=stopwords.words('indonesian')

# Set matplotlib parameters
mpl.rcParams['figure.figsize'] = (10, 10)
mpl.rcParams['font.size'] = 12
mpl.rcParams['figure.subplot.bottom'] = .1

# Generate word clouds
wordcloud_t = WordCloud(background_color='white', stopwords=sw).generate(' '.join(tb_plus['firman'].values))
wordcloud_t_stem = WordCloud(background_color='white', stopwords=sw).generate(' '.join(tb_plus['t_stem'].values))

# Create subplots
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(18, 12))
ax_flat = ax.flatten()
ax0 = ax_flat[0]
ax1 = ax_flat[1]

# Display word clouds
ax0.imshow(wordcloud_t, aspect="auto")
ax0.title.set_text('Untouched')
ax0.axis('off')

ax1.imshow(wordcloud_t_stem, aspect="auto")
ax1.title.set_text('Stemmed')
ax1.axis('off')

# Show plot
plt.show()

# Membagi dataframe menjadi Amsal dan Kisah Para Rasul
amsal = tb_plus[tb_plus['kitab'] == 'Ams']
kisah_para_rasul = tb_plus[tb_plus['kitab'] == 'Kis']

# Convert columns to string
amsal['t_stem_lem'] = amsal['t_stem_lem'].astype(str)
kisah_para_rasul['t_stem_lem'] = kisah_para_rasul['t_stem_lem'].astype(str)

# Generate wordcloud untuk Amsal dan Kisah Para Rasul
wordcloud_Amsal = WordCloud(background_color='white', stopwords=sw).generate(' '.join(amsal['t_stem_lem'].values))
wordcloud_KisahParaRasul = WordCloud(background_color='white', stopwords=sw).generate(' '.join(kisah_para_rasul['t_stem_lem'].values))

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))
ax0, ax1 = ax.flatten()

ax0.imshow(wordcloud_Amsal, aspect="auto")
ax0.title.set_text('Amsal')
ax0.axis('off')

ax1.imshow(wordcloud_KisahParaRasul, aspect="auto")
ax1.title.set_text('Kisah Para Rasul')
ax1.axis('off')

plt.show()

relevant_columns = tb_plus[['Waktu', 'NoWords', 'pasal']]
correlation_matrix = relevant_columns.corr(method='spearman')

tb_plus['panjangbab'] = round(tb_plus['NoWords'] / tb_plus['pasal'], 0)
tb_plus = tb_plus.sort_values(by='panjangbab', ascending=False)
df_untukdigabung = df.set_index('Kitab')
panjang_bab = tb_plus.merge(df_untukdigabung, left_on='kitab', right_on='Kitab', how='outer')
panjang_bab = panjang_bab.drop(['Versi_KingJames_y', 'Waktu_y', 'Periode_y', 'Lokasi_y'], axis=1)
panjang_bab.columns = panjang_bab.columns.str.replace('_x', '')
SpearmanCorr = panjang_bab[['NoWords', 'Waktu', 'panjangbab']].corr(method="spearman")

fig, axes = plt.subplots(ncols=2, figsize=(14, 7), dpi=80)
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[0])
axes[0].set_title('Spearman Correlation Heatmap dari Waktu, NoWords, dan Pasal')
sns.heatmap(SpearmanCorr, annot=True, cmap='YlGnBu', vmax=.9, square=True, linewidths=.3, fmt='.1f', ax=axes[1])
axes[1].set_title('Spearman Correlation Heatmap dari NoWords, Waktu, dan Panjang Bab')

plt.tight_layout()
plt.show()

PB = tb_plus[tb_plus['Perjanjian'] == 'Baru']
PL = tb_plus[tb_plus['Perjanjian'] == 'Lama']

# Fungsi untuk menghasilkan unigrams (n-grams dengan n=1)
def generate_ngrams(text, n=1):
    words = text.split()
    return [words[i:i+n] for i in range(len(words)-n+1)]

# Menghitung frekuensi unigrams
PB_unigrams = defaultdict(int)
PL_unigrams = defaultdict(int)

for line in PB['t_stem']:
    for word in generate_ngrams(line):
        PB_unigrams[word[0]] += 1

for line in PL['t_stem']:
    for word in generate_ngrams(line):
        PL_unigrams[word[0]] += 1

# Mengonversi hasil ke DataFrame dan mengurutkan berdasarkan frekuensi
df_PB_unigrams = pd.DataFrame(sorted(PB_unigrams.items(), key=lambda x: x[1], reverse=True), columns=['word', 'frequency'])
df_PL_unigrams = pd.DataFrame(sorted(PL_unigrams.items(), key=lambda x: x[1], reverse=True), columns=['word', 'frequency'])

# Menyiapkan plot
fig, axes = plt.subplots(ncols=2, figsize=(12, 10), dpi=80)
plt.tight_layout()

# Menentukan jumlah unigrams yang akan ditampilkan
N = 25

# Plotting top N unigrams untuk Perjanjian Baru
sns.barplot(y=df_PB_unigrams['word'].values[:N], x=df_PB_unigrams['frequency'].values[:N], ax=axes[0], color='red')
sns.barplot(y=df_PL_unigrams['word'].values[:N], x=df_PL_unigrams['frequency'].values[:N], ax=axes[1], color='green')

# Mengatur tampilan plot
for i in range(2):
    axes[i].spines['right'].set_visible(False)
    axes[i].set_xlabel('Frequency')
    axes[i].set_ylabel('Word')
    axes[i].tick_params(axis='x', labelsize=10)
    axes[i].tick_params(axis='y', labelsize=10)

axes[0].set_title(f'Top {N} Katap', fontsize=10)
axes[1].set_title(f'Top {N} most common unigrams in Old Testament', fontsize=10)

plt.show()

# Menghitung frekuensi bigrams
PB_bigrams = defaultdict(int)
PL_bigrams = defaultdict(int)

for line in PB['t_stem']:
    for bigram in generate_ngrams(line, n=2):
        PB_bigrams[' '.join(bigram)] += 1  # Join bigram elements into a string

for line in PL['t_stem']:
    for bigram in generate_ngrams(line, n=2):
        PL_bigrams[' '.join(bigram)] += 1  # Join bigram elements into a string

# Mengonversi hasil ke DataFrame dan mengurutkan berdasarkan frekuensi
df_PB_bigrams = pd.DataFrame(sorted(PB_bigrams.items(), key=lambda x: x[1], reverse=True), columns=['bigram', 'frequency'])
df_PL_bigrams = pd.DataFrame(sorted(PL_bigrams.items(), key=lambda x: x[1], reverse=True), columns=['bigram', 'frequency'])

# Menyiapkan plot untuk bigrams
fig, axes = plt.subplots(ncols=2, figsize=(12, 10), dpi=80)
plt.tight_layout()

# Plotting top N bigrams untuk Perjanjian Baru dan Perjanjian Lama
sns.barplot(y=df_PB_bigrams['bigram'].values[:N], x=df_PB_bigrams['frequency'].values[:N], ax=axes[0], color='red')
sns.barplot(y=df_PL_bigrams['bigram'].values[:N], x=df_PL_bigrams['frequency'].values[:N], ax=axes[1], color='green')

# Mengatur tampilan plot
for i in range(2):
    axes[i].spines['right'].set_visible(False)
    axes[i].set_xlabel('Frequency')
    axes[i].set_ylabel('Bigram')
    axes[i].tick_params(axis='x', labelsize=10)
    axes[i].tick_params(axis='y', labelsize=10)

axes[0].set_title(f'Top {N} bigrams in New Testament', fontsize=10)
axes[1].set_title(f'Top {N} bigrams in Old Testament', fontsize=10)

plt.show()

# Inisialisasi tokenizer dan model BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model = BertModel.from_pretrained('bert-base-multilingual-cased')

# Fungsi untuk mendapatkan embedding dari BERT
def get_bert_embedding(text, tokenizer, model):
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()
    return cls_embedding

# Mengambil embedding untuk setiap ayat dalam dataset
tb_plus['bert_embedding'] = tb_plus['t_stem'].apply(lambda x: get_bert_embedding(x, tokenizer, model))

# Misalkan kita akan mengklasifikasikan Perjanjian Lama atau Baru
tb_plus['label'] = tb_plus['Perjanjian'].apply(lambda x: 1 if x == 'Baru' else 0)

# Pisahkan fitur (embedding) dan label
X = np.array(tb_plus['bert_embedding'].tolist())
y = tb_plus['label'].values

# Bagi data menjadi training dan testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Inisialisasi model Logistic Regression
model = LogisticRegression(max_iter=1000)

# Latih model
model.fit(X_train, y_train)

# Prediksi pada data testing
y_pred = model.predict(X_test)

# Evaluasi model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')

# Print classification report
print(classification_report(y_test, y_pred))

# Inisialisasi tokenizer dan model BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model = BertModel.from_pretrained('bert-base-multilingual-cased')

# Fungsi untuk mendapatkan embedding dari BERT
def get_bert_embedding(text, tokenizer, model):
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()
    return cls_embedding

# Mengambil embedding untuk setiap ayat dalam dataset
tb_plus['bert_embedding'] = tb_plus['t_stem'].apply(lambda x: get_bert_embedding(x, tokenizer, model))

# Misalkan kita akan mengklasifikasikan Perjanjian Lama atau Baru
tb_plus['label'] = tb_plus['Perjanjian'].apply(lambda x: 1 if x == 'Baru' else 0)

# Pisahkan fitur (embedding) dan label
X = np.array(tb_plus['bert_embedding'].tolist())
y = tb_plus['label'].values

# Bagi data menjadi training dan testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Inisialisasi model Logistic Regression
model = LogisticRegression(max_iter=1000)

# Latih model
model.fit(X_train, y_train)

# Prediksi pada data testing
y_pred = model.predict(X_test)

# Evaluasi model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')

# Print classification report
print(classification_report(y_test, y_pred))

